{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTSpotify EDA\n",
    "\n",
    "Here we will use the spotify api and analyze the top songs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials  # used to login to spotify client\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logining into the spotify api\n",
    "\n",
    "cid = 'd328d69f743743ae9784dbc555e47a85'  # Client ID Login\n",
    "secret = '9a363b6c67d2445fa072c7820fcc8c43'  # Client Secret #\n",
    "\n",
    "client_credentials_manager = SpotifyClientCredentials(client_id=cid, client_secret=secret)\n",
    "sp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# genre =[]\n",
    "# test=pd.DataFrame()\n",
    "listyear =[]\n",
    "tracklist =[]\n",
    "# Using the Spotipy Library and Spotify API to get data from a range of years. \n",
    "def get_spotify (start_year, end_year):\n",
    "    years_int = np.arange(int(start_year),int(end_year)+1)  # sets the interested range of years we're interested in\n",
    "    for year in years_int:\n",
    "        artist_name = []\n",
    "        track_name = []\n",
    "        popularity = []\n",
    "        song_id = []\n",
    "\n",
    "    #     print(year)\n",
    "        y = 'year:{}'.format(year)\n",
    "    #     print(y)\n",
    "    #     test.to_csv('tracklist{}'.format(year))\n",
    "        for i in range (0,500, 50):  # sets loops to loop through entires in limits of 50\n",
    "        #     print(i)\n",
    "            results = sp.search(q=y, type='track', limit=50, offset=i)  # specifies the query in a dictionary, type, limit, and offests of api\n",
    "        #     artists_info = results['tracks']['items'][0]['artists']\n",
    "        #     artists_id = str(artists_info).split(\"'id': '\",1)[1].split(\"', 'name\")[0]\n",
    "        #     genre.append(sp.artist(artist_id)['genres'])\n",
    "            for index, track in enumerate(results['tracks']['items']):\n",
    "                song_id.append(track['id'])\n",
    "        #         genre.append(sp.artist(artist_id)['genres'])\n",
    "                artist_name.append(track['artists'][0]['name'])\n",
    "                track_name.append(track['name'])\n",
    "    \n",
    "                popularity.append(track['popularity'])\n",
    "        #         genre.append(track['genres'])\n",
    "    \n",
    "        tracks = pd.DataFrame({'song_id':song_id, 'artist_name':artist_name, \n",
    "                           'track_name':track_name,'popularity':popularity, 'year':year})\n",
    "    \n",
    "        tracks.to_csv('tracklist{}.csv'.format(year), index=False)\n",
    "        tracklist.append('tracklist{}.csv'.format(year))\n",
    "        listyear.append('tracks{}'.format(year))\n",
    "    \n",
    "    return(listyear, tracklist)\n",
    "# # #         print(index, track)\n",
    "# # #     song_id.append(results['tracks']['items'][i]['id'])  # indexes returned dictionary to find the song id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tracklist2010.csv is being read into variable tracks2010\n",
      "tracklist2011.csv is being read into variable tracks2011\n",
      "tracklist2012.csv is being read into variable tracks2012\n",
      "tracklist2013.csv is being read into variable tracks2013\n",
      "tracklist2014.csv is being read into variable tracks2014\n",
      "tracklist2015.csv is being read into variable tracks2015\n",
      "tracklist2016.csv is being read into variable tracks2016\n",
      "tracklist2017.csv is being read into variable tracks2017\n",
      "tracklist2018.csv is being read into variable tracks2018\n",
      "tracklist2019.csv is being read into variable tracks2019\n",
      "(5000, 5)\n",
      "(500, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song_id</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>track_name</th>\n",
       "      <th>popularity</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>7DfFc7a6Rwfi3YQMRbDMau</td>\n",
       "      <td>Frank Ocean</td>\n",
       "      <td>Thinkin Bout You</td>\n",
       "      <td>74</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>6FE2iI43OZnszFLuLtvvmg</td>\n",
       "      <td>MKTO</td>\n",
       "      <td>Classic</td>\n",
       "      <td>76</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>0nJW01T7XtvILxQgC5J7Wh</td>\n",
       "      <td>Bruno Mars</td>\n",
       "      <td>When I Was Your Man</td>\n",
       "      <td>81</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>3bidbhpOYeV4knp8AIu8Xn</td>\n",
       "      <td>Macklemore &amp; Ryan Lewis</td>\n",
       "      <td>Can't Hold Us - feat. Ray Dalton</td>\n",
       "      <td>82</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>4G8gkOterJn0Ywt6uhqbhp</td>\n",
       "      <td>Imagine Dragons</td>\n",
       "      <td>Radioactive</td>\n",
       "      <td>72</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     song_id              artist_name  \\\n",
       "1000  7DfFc7a6Rwfi3YQMRbDMau              Frank Ocean   \n",
       "1001  6FE2iI43OZnszFLuLtvvmg                     MKTO   \n",
       "1002  0nJW01T7XtvILxQgC5J7Wh               Bruno Mars   \n",
       "1003  3bidbhpOYeV4knp8AIu8Xn  Macklemore & Ryan Lewis   \n",
       "1004  4G8gkOterJn0Ywt6uhqbhp          Imagine Dragons   \n",
       "\n",
       "                            track_name  popularity  year  \n",
       "1000                  Thinkin Bout You          74  2012  \n",
       "1001                           Classic          76  2012  \n",
       "1002               When I Was Your Man          81  2012  \n",
       "1003  Can't Hold Us - feat. Ray Dalton          82  2012  \n",
       "1004                       Radioactive          72  2012  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_year=2010\n",
    "end_year =2019\n",
    "get_spotify(start_year, end_year)\n",
    "\n",
    "# loading the data into variables \n",
    "for x, y in zip(listyear,tracklist):\n",
    "    print(y,'is being read into variable', x)\n",
    "    globals()[x] = pd.read_csv(y)  # sets a global variable based on the name of the trackyear \n",
    "\n",
    "## Can also load and merge all the same set of code also seen below\n",
    "\n",
    "dfs = [pd.read_csv(filename, index_col ='song_id') for filename in tracklist]  # loads all the data into one large list instead of individual dataframes\n",
    "                                                             # from these lists we could further break down into individual dataframes if needed\n",
    "\n",
    "df = pd.concat((df for df in dfs), axis=0).reset_index()  # with all the years in one dataframe we can mask by year if nesesary\n",
    "df.to_csv('TrackInfoFrom{}-{}.csv'.format(start_year, end_year), index=False)  # Save to output\n",
    "\n",
    "print(df.shape)  # looks like we have the right ammout of rows!\n",
    "print(df[df['year']==2012].shape)  # doublechecking we have the correct number of rows\n",
    "df[df['year']==2012].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song_id</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>track_name</th>\n",
       "      <th>popularity</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4HlFJV71xXKIGcU3kRyttv</td>\n",
       "      <td>Train</td>\n",
       "      <td>Hey, Soul Sister</td>\n",
       "      <td>82</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7BqBn9nzAq8spo5e7cZ0dJ</td>\n",
       "      <td>Bruno Mars</td>\n",
       "      <td>Just the Way You Are</td>\n",
       "      <td>79</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>03tqyYWC9Um2ZqU0ZN849H</td>\n",
       "      <td>Waka Flocka Flame</td>\n",
       "      <td>No Hands (feat. Roscoe Dash &amp; Wale)</td>\n",
       "      <td>73</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2qYsSHsYkihWx043HVJQRV</td>\n",
       "      <td>Jason Aldean</td>\n",
       "      <td>Dirt Road Anthem</td>\n",
       "      <td>72</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15JINEqzVMv3SvJTAXAKED</td>\n",
       "      <td>Eminem</td>\n",
       "      <td>Love The Way You Lie</td>\n",
       "      <td>81</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  song_id        artist_name  \\\n",
       "0  4HlFJV71xXKIGcU3kRyttv              Train   \n",
       "1  7BqBn9nzAq8spo5e7cZ0dJ         Bruno Mars   \n",
       "2  03tqyYWC9Um2ZqU0ZN849H  Waka Flocka Flame   \n",
       "3  2qYsSHsYkihWx043HVJQRV       Jason Aldean   \n",
       "4  15JINEqzVMv3SvJTAXAKED             Eminem   \n",
       "\n",
       "                            track_name  popularity  year  \n",
       "0                     Hey, Soul Sister          82  2012  \n",
       "1                 Just the Way You Are          79  2012  \n",
       "2  No Hands (feat. Roscoe Dash & Wale)          73  2012  \n",
       "3                     Dirt Road Anthem          72  2012  \n",
       "4                 Love The Way You Lie          81  2012  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracks2012.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 5)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracks.shape  # 2/3 not seeming to get the for loop right when looping through multiple years "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data \n",
    "\n",
    "# for year in years_int:\n",
    "for x, y in zip(trackyears,tracklist):\n",
    "    print(y)\n",
    "    globals()[x] = pd.read_csv(y)  # sets a global variable based on the name of the trackyear \n",
    "\n",
    "# Can also load and merge all the same set of code also seen below\n",
    "\n",
    "dfs = [pd.read_csv(filename, index_col ='song_id') for filename in tracklist]  # loads all the data into one large list instead of individual dataframes\n",
    "                                                         # from these lists we could further break down into individual dataframes if needed\n",
    "    \n",
    "# # to join them all into one dataframe we can join the lists to a dataframe using an outer join or taking everything from each list that does not match the previous list\n",
    "# len(years_int)\n",
    "# df = pd.DataFrame\n",
    "# for i in range(len(years_int)):\n",
    "#     print(i)\n",
    "#     while i < len(years_int):\n",
    "#         df = dfs[i],\n",
    "# # df = pd.DataFrame().join(dfs, how='outer')\n",
    "\n",
    "# using the reduce function from the functools library\n",
    "from functools import reduce \n",
    "# df_final = dfs[0].join(dfs[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks2010.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the SHape we see that there are more rows of data than there are in indiviual files. WE have to remember "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(0,10,1):\n",
    "    print(len(dfs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfs[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks2019.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2010.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spotifys api has a limit of 100 when trying to find audio features like dancibility and energy we will write a for loop\n",
    "features = []\n",
    "none_counter = 0\n",
    "for i in range(0,len(song_id),100):    \n",
    "    audio_features = sp.audio_features(song_id[i:i+100])\n",
    "#     print(audio_features)\n",
    "    for index, track in enumerate(audio_features):\n",
    "        if track == None:\n",
    "            none_counter = none_counter + 1\n",
    "        else:\n",
    "            features.append(track)  \n",
    "\n",
    "features = pd.DataFrame.from_dict(features, orient='columns')\n",
    "features.to_csv('song_features.csv')\n",
    "print('Shape before removing text columns{}'.format(features.shape))\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.DataFrame.from_dict(features, orient='columns')\n",
    "print('Shape before removing text columns{}'.format(features.shape))\n",
    "features.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning up the data removing unnecessary cols and checking for nuls ets\n",
    "features['song_id'] = features['id']\n",
    "features = features.drop(['analysis_url','track_href','type', 'uri', 'id'], axis=1)\n",
    "print('Shape after removing columns{}'.format(features.shape))\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging both dataframes \n",
    "merged_tracks = tracks.merge(features, on='song_id', copy =False)\n",
    "# merged_tracks.drop('id', axis=1, inplace=True)\n",
    "print('Shape of Merged tracks', merged_tracks.shape)\n",
    "merged_tracks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates even though there shouldn't be \n",
    "merged_tracks.drop_duplicates(subset=['artist_name','track_name'], inplace=True)\n",
    "print('Shape after dropping duplicates', merged_tracks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can save this file now that we have it for use in other places\n",
    "merged_tracks.to_csv('2019TopTracks.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# lets look into the correlation map to see if we cna any correations to popularity\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "corr = merged_tracks.corr()\n",
    "sns.heatmap(corr, annot=True, fmt='.2f', annot_kws={'size': 10}, square=True)\n",
    "\n",
    "# to fix the half box error from sns heatmap we have to adjust the ax limits using the code below\n",
    "bottom, top = ax.get_ylim()  \n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above we are able to see the most that dancibility energy loudness and valence seem to have to greatest correlation. We can plot this in a scatter plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_features = merged_tracks[['popularity','loudness','energy','danceability','valence']]\n",
    "sns.pairplot(int_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see some interesting relationships. For example looking at loudness the songs that seem to be quiter are not as popular as those that are. We can also see that although alot of the songs are pretty danceable there are also alot of non popular songs.\n",
    "\n",
    "After observing we can use these features to predict the popularity of a song using Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning \n",
    "\n",
    "Using maching learning to predict popularity based on feautres such as dancibility etc. We will use three models to predict and compare the accuracy of our models. We will use, KNN\n",
    "SVM\n",
    "Decision Tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K - Nearest Neighbors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "regressor = DecisionTreeRegressor(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = int_features.drop('popularity', axis=1)\n",
    "label = int_features['popularity']\n",
    "X_train, x_test, y_train, y_test = train_test_split(features, label, test_size=0.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KnnR = KNeighborsRegressor(n_neighbors=10)\n",
    "KnnR.fit(X_train, y_train)\n",
    "pred = KnnR.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KnnR.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KnnR.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 44  # random seed\n",
    "## initializing regression models \n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
    "# from xgboost import XGBRegressor\n",
    "# from lightgbm import LGBMRegressor\n",
    "\n",
    "''''We are interested in the following 14 regression models.\n",
    "All initialized with default parameters except random_state and n_jobs.'''\n",
    "lr = LinearRegression(n_jobs = -1)\n",
    "lasso = Lasso(random_state = seed)\n",
    "ridge = Ridge(random_state = seed)\n",
    "elnt = ElasticNet(random_state = seed)\n",
    "kr = KernelRidge()\n",
    "dt = DecisionTreeRegressor(random_state = seed)\n",
    "svr = SVR()\n",
    "knn = KNeighborsRegressor(n_jobs= -1)\n",
    "pls = PLSRegression()\n",
    "rf = RandomForestRegressor(n_jobs = -1, random_state = seed)\n",
    "et = ExtraTreesRegressor(n_jobs = -1, random_state = seed)\n",
    "ab = AdaBoostRegressor(random_state = seed)\n",
    "gb = GradientBoostingRegressor(random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# x_test = scaler.fit_transform(x_test)\n",
    "# y_train = scaler.fit_transform(y_train.reshape(-1,1))\n",
    "# y_test = scaler.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to return accuracy by default score method that reurns R^2\n",
    "def r2_train(model):\n",
    "    model.fit(X_train, y_train)\n",
    "    return model.score(X_train, y_train)\n",
    "\n",
    "## calculating R^2 score for all the models in a for loop\n",
    "models = [lr, lasso, ridge, elnt, kr, dt, svr, knn, pls, rf, et, ab, gb]\n",
    "training_score = []\n",
    "for model in models:\n",
    "    training_score.append(r2_train(model))\n",
    "\n",
    "## plotting training accuracy for each model \n",
    "train_score = pd.DataFrame(data = training_score, columns = ['Training R^2'])\n",
    "## naming the index column values according to model type\n",
    "train_score.index = ['LR', 'LASSO',  'RIDGE', 'ELNT', 'KR', 'DT', 'SVR', 'KNN', 'PLS', 'RF', 'ET', 'AB', 'GB']\n",
    "\n",
    "train_score = (train_score*100).round(4)\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "ax.scatter(train_score.index, train_score['Training R^2'], s = 200, c='g')\n",
    "plt.xticks(rotation = 90)\n",
    "ax.set_xlabel('Models',  fontsize=14)\n",
    "ax.set_ylabel('% Training Score',  fontsize=14)\n",
    "ax.set_title('Training Score (R^2) vs Model',  fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = dt.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.arange(len(y_test)), y_test, alpha=0.3)\n",
    "plt.scatter(np.arange(len(y_test)), pred, color='r', alpha=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
